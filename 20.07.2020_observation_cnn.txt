---------------------------------------------
--------------- Ziele bzw. ToDo -------------
---------------------------------------------

Ziele: 
- Accuracy erhöhen

ToDo:
- Datenmenge anpassen:
    - Songs die das falsche genre im Datensatz haben in das richtige genre stecken, falls vorhanden
    - Duplikate entfernen
    - Anschließend zählen wieviele songs in einem genre übrig bleiben
    - Anschließend nochmal Mfccs berechnen, mit 10 Segmenten und durch Model klassifizieren lassen
    - mit t-SNE visualisieren
- Modell
    - Error-Rate wieder reinkodieren
    - Netzanpassungen: 
        - Anzahl Filter erhöhen
        - Noch ein Conv2D einbauen
        - Noch ein Dense Layer mit 128 vor 64 setzen bzw, 64 durch 128 Dense ersetzen
    - mind. 10 Durchläufe für jedes CNN machen, falls Hyperparameter geändert
        - Acc/Error pro Durchlauf notieren, Konfusionsmatrix/Graph FÜR JEDEN DURCHLAUF PROTOKOLLIEREN!!!
- Weitere Features extrahieren und an die Inputdaten anhängen und diese dann durch Model klassifizieren lassen:
    - Spectral Centroid
    - Zero Crossing Rate
    - Spectral Rolloff
    - Chroma Frequencies




---------------------------------------------
------- Ergebnisse und Beobachtungen --------
---------------------------------------------

- Accuracy auf Testdaten:
    1) 0.7227

- Falsche Zuordnungen
    - Country <-> Reggae: 35
    - Reggae <-> HipHop: 31
    - Country <-> Blues: 28

- Folgender Block ist auffällig mit vielen Falschzuordnungen:
    - Country, Disco, Hiphop - Pop, reggae, rock

- Graph - Trainingsverlauf
    - Ab Epoche 15 findet eine sehr leichte Überanpassung (Overfit) statt
        - test_accuracy steigt ab dort kaum noch an

---------------------------------------------
--------------- Hyperparameter --------------
---------------------------------------------
Feature Extraktion:

- n_mfcc = 13         -> Anzahl an mfcc koeffizienten
- n_fft = 2048        -> Breite des Fensters bei der Fourier Transformation
- hop_length = 512
- num_segments = 10
- Data Augmentation: 
    - Jeder Song wurde in num_segments gleich lange Segmente unterteilt
    - für jeden Abschnitt wurden je n_mfccs berechnet
    - Aus den ursprünglich 1000 Samples wurden also 10000 augmentierte Samples

Model:

anzahl cnn filter = 32
kernelsize = (3, 3) bzw. (2, 2) im 3. Conv2D Layer
test_split = 0.25                                   # proportion of training data used for test
validation_ratio = 0.2                              # proportion of training data used for validation
sgd optimizer = Adam
learning_rate = 0.0001
batch_size = 32
epochs = 30
loss = sparse_categorical_crossentropy              # = Kreuzentropie. Keine onehot Kodierung, sonst categorical_crossentropy
metrics = accuracy                                  # anzahl korrekte Vorhersagen / anzahl der daten
Regularisierung durch
    - Dropout Layer mit p=0.3


---------------------------------------------
------ Verwendetes CNN - Modell -------
---------------------------------------------

model = keras.models.Sequential([
    # 1. Input Conv Layer = (anzahl mfcc vektoren, n mfcc Koeffizienten, 1) 
    keras.layers.Conv2D(32, 3, activation='relu', input_shape=cnn_input_shape),
    keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'),
    keras.layers.BatchNormalization(),
    
    # 2. Hidden Conv Layer
    keras.layers.Conv2D(32, 3, padding='same', activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'),
    keras.layers.BatchNormalization(),

    # 3. Hidden Conv Layer
    # -> kleinere Kernelsize in Conv2D
    # -> kleinere Poolsize in MaxPooling2D
    keras.layers.Conv2D(32, 2, padding='same', activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'),    
    keras.layers.BatchNormalization(),

    # 4. Flatten und Dense Layer
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.3),

    # 5. Output Layer
    keras.layers.Dense(10, activation='softmax') 
])



